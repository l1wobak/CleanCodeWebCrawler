# Java Web Crawler

A simple and configurable web crawler written in Java for the Clean Code assignment.
It recursively crawls a given website and its links (limited by depth and domain), collecting page headings and links. The output is saved as a Markdown report.

---

## Prerequisites

- Java 17+
- Maven 3+

---

## How to Build

From the project root, run:

```
mvn clean package
```

This will compile the crawler and package it into a runnable JAR in the `target/` directory.

---

## How to Run

The crawler requires **3 arguments**:

1. **Start URL** – The URL where crawling begins
2. **Depth** – How deep the crawler should traverse links
3. **Allowed Domains** – A comma-separated list of domains to restrict crawling to

### Command Format

```
java -jar target/java-web-crawler-1.0-SNAPSHOT.jar <StartUrl> <depth> <domain1,domain2,...>
```

### Example

```
java -jar target/java-web-crawler-1.0-SNAPSHOT.jar https://gilead-verein.at/ 2 https://gilead-verein.at/,http://www.klagenfurt.at/,https://www.stw.at
```

This command will crawl the given page up to depth `2`, staying within the specified domains.
An example output can be found in `report.md`.

---

## Output

The crawler generates a file called `report.md` in the directory where the program is started.
It contains:

- A list of all crawled pages
- Their headings (H1–H6)
- All extracted links
- Highlighted broken links (marked with ❌)

---

## Testing

To run the automated unit tests:

```
mvn test
```

---

## Author

This project was created by Lukas Wobak as part of the Clean Code course assignment.

# ğŸ•¸ï¸ Java Web Crawler

A simple and configurable web crawler written in Java for the Clean Code assignment.
It recursively crawls a given website and its links (limited by depth and domain), collecting page headings and links. The output is saved as a Markdown report.

---

## ğŸ“¦ Prerequisites

- Java 17+
- Maven 3+

---

## ğŸš€ How to Build

From the project root, run:

```
mvn clean package
```

This will compile the crawler and package it into a runnable JAR in the `target/` directory.

---

## â–¶ï¸ How to Run

The crawler requires **3 arguments**:

1. **Start URL** â€“ The URL where crawling begins
2. **Depth** â€“ How deep the crawler should traverse links
3. **Allowed Domains** â€“ A comma-separated list of domains to restrict crawling to

### ğŸ“Œ Command Format

```
java -jar target/java-web-crawler-1.0-SNAPSHOT.jar <StartUrl> <depth> <domain1,domain2,...>
```

### âœ… Example

```
java -jar target/java-web-crawler-1.0-SNAPSHOT.jar https://gilead-verein.at/ 2 https://gilead-verein.at/,http://www.klagenfurt.at/,https://www.stw.at
```

This command will crawl the given page up to depth `2`, staying within the specified domains.
An example output can be found in `report.md`.

---

## ğŸ“„ Output

The crawler generates a file called `report.md` in the directory where the program is started.
It contains:

- A list of all crawled pages
- Their headings (H1â€“H6)
- All extracted links
- Highlighted broken links (marked with âŒ)

---

## ğŸ§ª Testing

To run the automated unit tests:

```
mvn test
```

---

## ğŸ‘¤ Author

This project was created by **Lukas Wobak** as part of the Clean Code course assignment.
